{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**載入文字資料**"
      ],
      "metadata": {
        "id": "XVw-EIUCYeCp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "hCh2jNVJYYP-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class DataLoader():\n",
        "  def __init__(self):\n",
        "    path = tf.keras.utils.get_file('nietzsche.txt',origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "\n",
        "    with open(path,encoding='utf-8') as f:\n",
        "      self.raw_text = f.read().lower()\n",
        "\n",
        "    self.chars = sorted(list(set(self.raw_text)))\n",
        "    self.char_indices = dict((c,i) for i, c in enumerate(self.chars))\n",
        "    self.indices_char = dict((i,c) for i, c in enumerate(self.chars))\n",
        "    self.text = [self.char_indices[c] for c in self.raw_text]\n",
        "\n",
        "  def get_batch(self, seq_length, batch_size):\n",
        "    seq = []\n",
        "    next_char = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      index = np.random.randint(0, len(self.text) - seq_length)\n",
        "      seq.append(self.text[index:index+seq_length])\n",
        "      next_char.append(self.text[index+seq_length])\n",
        "\n",
        "      return np.array(seq), np.array(next_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**建立模型**"
      ],
      "metadata": {
        "id": "kiBkMlAMfQdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.multiarray import dtype\n",
        "class RNN(tf.keras.Model):\n",
        "  def __init__(self, num_chars, batch_size, seq_length):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_chars = num_chars\n",
        "    self.seq_length = seq_length\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.cell = tf.keras.layers.LSTMCell(units=256)\n",
        "    self.dense  = tf.keras.layers.Dense(units=self.num_chars)\n",
        "\n",
        "  def call(self, inputs, from_logits=False):\n",
        "    inputs = tf.one_hot(inputs, depth=self.num_chars)\n",
        "    state = self.cell.get_initial_state(batch_size=self.batch_size,dtype=tf.float32)\n",
        "\n",
        "    for t in range(self.seq_length):\n",
        "      output, state = self.cell(inputs[:,t,:],state)\n",
        "\n",
        "    logits = self.dense(output)\n",
        "\n",
        "    if from_logits:\n",
        "      return logits\n",
        "    else:\n",
        "      return tf.nn.softmax(logits)\n",
        "\n"
      ],
      "metadata": {
        "id": "H0c53T_9fV2r"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**參數設定**"
      ],
      "metadata": {
        "id": "CPXp1d_oTAfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = 1000\n",
        "seq_length = 40\n",
        "batch_size = 1\n",
        "learning_rate = 1e-3"
      ],
      "metadata": {
        "id": "UpaZ7cqpTFgI"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **訓練開始**"
      ],
      "metadata": {
        "id": "mYckaA9jTnzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.engine.training import optimizer\n",
        "data_loader = DataLoader()\n",
        "model = RNN(num_chars=len(data_loader.chars),batch_size=batch_size,seq_length=seq_length)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "for batch_index in range(num_batches):\n",
        "  X, y = data_loader.get_batch(seq_length,batch_size)\n",
        "  with tf.GradientTape() as tape:\n",
        "    y_pred = model(X)\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y,y_pred=y_pred)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    print(\"batch %d loss %f\" % (batch_index,loss.numpy()))\n",
        "  grads = tape.gradient(loss,model.variables)\n",
        "  optimizer.apply_gradients(grads_and_vars=zip(grads,model.variables))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyPqhzG2TuxE",
        "outputId": "61377d44-abd0-4db2-d175-28ed6b830d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0 loss 4.057539\n",
            "batch 1 loss 4.075435\n",
            "batch 2 loss 4.075804\n",
            "batch 3 loss 4.040296\n",
            "batch 4 loss 3.993575\n",
            "batch 5 loss 4.045423\n",
            "batch 6 loss 3.972641\n",
            "batch 7 loss 3.902284\n",
            "batch 8 loss 4.059404\n",
            "batch 9 loss 4.077812\n",
            "batch 10 loss 4.126665\n",
            "batch 11 loss 4.059216\n",
            "batch 12 loss 4.065313\n",
            "batch 13 loss 3.990505\n",
            "batch 14 loss 3.793549\n",
            "batch 15 loss 4.105025\n",
            "batch 16 loss 3.806931\n",
            "batch 17 loss 4.053829\n",
            "batch 18 loss 3.636839\n",
            "batch 19 loss 4.183213\n",
            "batch 20 loss 3.140736\n",
            "batch 21 loss 3.822880\n",
            "batch 22 loss 2.644754\n",
            "batch 23 loss 5.376073\n",
            "batch 24 loss 3.918265\n",
            "batch 25 loss 3.955055\n",
            "batch 26 loss 5.548432\n",
            "batch 27 loss 1.757520\n",
            "batch 28 loss 4.011651\n",
            "batch 29 loss 1.776685\n",
            "batch 30 loss 5.233916\n",
            "batch 31 loss 3.922043\n",
            "batch 32 loss 3.799827\n",
            "batch 33 loss 3.142810\n",
            "batch 34 loss 3.323278\n",
            "batch 35 loss 2.945462\n",
            "batch 36 loss 1.839828\n",
            "batch 37 loss 1.785256\n",
            "batch 38 loss 3.579210\n",
            "batch 39 loss 3.371750\n",
            "batch 40 loss 2.546045\n",
            "batch 41 loss 3.288194\n",
            "batch 42 loss 4.705637\n",
            "batch 43 loss 3.117269\n",
            "batch 44 loss 2.894966\n",
            "batch 45 loss 4.309769\n",
            "batch 46 loss 2.249304\n",
            "batch 47 loss 2.572553\n",
            "batch 48 loss 2.152951\n",
            "batch 49 loss 3.700398\n",
            "batch 50 loss 2.444257\n",
            "batch 51 loss 1.970908\n",
            "batch 52 loss 3.463402\n",
            "batch 53 loss 1.756348\n",
            "batch 54 loss 4.730917\n",
            "batch 55 loss 3.833987\n",
            "batch 56 loss 2.049668\n",
            "batch 57 loss 3.433604\n",
            "batch 58 loss 1.313296\n",
            "batch 59 loss 3.116979\n",
            "batch 60 loss 3.153304\n",
            "batch 61 loss 2.999351\n",
            "batch 62 loss 2.750514\n",
            "batch 63 loss 3.374194\n",
            "batch 64 loss 6.953644\n",
            "batch 65 loss 6.698396\n",
            "batch 66 loss 4.022399\n",
            "batch 67 loss 2.576326\n",
            "batch 68 loss 6.693639\n",
            "batch 69 loss 1.975452\n",
            "batch 70 loss 3.840223\n",
            "batch 71 loss 2.225611\n",
            "batch 72 loss 5.716721\n",
            "batch 73 loss 2.380578\n",
            "batch 74 loss 2.919447\n",
            "batch 75 loss 5.460105\n",
            "batch 76 loss 2.658755\n",
            "batch 77 loss 2.721472\n",
            "batch 78 loss 3.916785\n",
            "batch 79 loss 2.785404\n",
            "batch 80 loss 3.237511\n",
            "batch 81 loss 2.690474\n",
            "batch 82 loss 3.504146\n",
            "batch 83 loss 4.429423\n",
            "batch 84 loss 5.437500\n",
            "batch 85 loss 2.503067\n",
            "batch 86 loss 2.470993\n",
            "batch 87 loss 3.348174\n",
            "batch 88 loss 5.363678\n",
            "batch 89 loss 4.249437\n",
            "batch 90 loss 3.460888\n",
            "batch 91 loss 2.586339\n",
            "batch 92 loss 5.093591\n",
            "batch 93 loss 2.988667\n",
            "batch 94 loss 3.073667\n",
            "batch 95 loss 3.299828\n",
            "batch 96 loss 2.965675\n",
            "batch 97 loss 2.413456\n",
            "batch 98 loss 2.605366\n",
            "batch 99 loss 4.649302\n",
            "batch 100 loss 2.322672\n",
            "batch 101 loss 2.427057\n",
            "batch 102 loss 3.216615\n",
            "batch 103 loss 2.179323\n",
            "batch 104 loss 3.635946\n",
            "batch 105 loss 2.176989\n",
            "batch 106 loss 6.410307\n",
            "batch 107 loss 4.390856\n",
            "batch 108 loss 2.968594\n",
            "batch 109 loss 4.130653\n",
            "batch 110 loss 1.935205\n",
            "batch 111 loss 3.094377\n",
            "batch 112 loss 1.941678\n",
            "batch 113 loss 2.351958\n",
            "batch 114 loss 2.342701\n",
            "batch 115 loss 1.826229\n",
            "batch 116 loss 2.422990\n",
            "batch 117 loss 2.196450\n",
            "batch 118 loss 2.147433\n",
            "batch 119 loss 1.698129\n",
            "batch 120 loss 3.351428\n",
            "batch 121 loss 3.320647\n",
            "batch 122 loss 1.979936\n",
            "batch 123 loss 1.657028\n",
            "batch 124 loss 3.735442\n",
            "batch 125 loss 3.024246\n",
            "batch 126 loss 4.903419\n",
            "batch 127 loss 1.754768\n",
            "batch 128 loss 2.646484\n",
            "batch 129 loss 3.627308\n",
            "batch 130 loss 3.559188\n",
            "batch 131 loss 2.092997\n",
            "batch 132 loss 1.738838\n",
            "batch 133 loss 2.356210\n",
            "batch 134 loss 3.167620\n",
            "batch 135 loss 3.125250\n",
            "batch 136 loss 7.346120\n",
            "batch 137 loss 3.614075\n",
            "batch 138 loss 1.901249\n",
            "batch 139 loss 2.155764\n",
            "batch 140 loss 2.786094\n",
            "batch 141 loss 2.140491\n",
            "batch 142 loss 7.197621\n",
            "batch 143 loss 2.022361\n",
            "batch 144 loss 4.024978\n",
            "batch 145 loss 1.994076\n",
            "batch 146 loss 1.931249\n",
            "batch 147 loss 2.255861\n",
            "batch 148 loss 4.253755\n",
            "batch 149 loss 3.371803\n",
            "batch 150 loss 6.465051\n",
            "batch 151 loss 2.118693\n",
            "batch 152 loss 6.310625\n",
            "batch 153 loss 3.345976\n",
            "batch 154 loss 4.380371\n",
            "batch 155 loss 2.089406\n",
            "batch 156 loss 2.350544\n",
            "batch 157 loss 4.116759\n",
            "batch 158 loss 2.041368\n",
            "batch 159 loss 2.724242\n",
            "batch 160 loss 3.240978\n",
            "batch 161 loss 5.195472\n",
            "batch 162 loss 2.649567\n",
            "batch 163 loss 3.404051\n",
            "batch 164 loss 3.598114\n",
            "batch 165 loss 2.005647\n",
            "batch 166 loss 3.832420\n",
            "batch 167 loss 2.496241\n",
            "batch 168 loss 3.553766\n",
            "batch 169 loss 2.949640\n",
            "batch 170 loss 1.938971\n",
            "batch 171 loss 1.865218\n",
            "batch 172 loss 2.329789\n",
            "batch 173 loss 4.670635\n",
            "batch 174 loss 3.113862\n",
            "batch 175 loss 3.563776\n",
            "batch 176 loss 2.627542\n",
            "batch 177 loss 2.627308\n",
            "batch 178 loss 2.945123\n",
            "batch 179 loss 3.920151\n",
            "batch 180 loss 2.949821\n",
            "batch 181 loss 1.664325\n",
            "batch 182 loss 2.382478\n",
            "batch 183 loss 2.875494\n",
            "batch 184 loss 2.216722\n",
            "batch 185 loss 5.786093\n",
            "batch 186 loss 5.143337\n",
            "batch 187 loss 1.776056\n",
            "batch 188 loss 5.506704\n",
            "batch 189 loss 6.971968\n",
            "batch 190 loss 3.395061\n",
            "batch 191 loss 5.555142\n",
            "batch 192 loss 2.619569\n",
            "batch 193 loss 1.886025\n",
            "batch 194 loss 2.537081\n",
            "batch 195 loss 3.158297\n",
            "batch 196 loss 2.122312\n",
            "batch 197 loss 4.604564\n",
            "batch 198 loss 2.424336\n",
            "batch 199 loss 2.864529\n",
            "batch 200 loss 2.339861\n",
            "batch 201 loss 4.543365\n",
            "batch 202 loss 3.275820\n",
            "batch 203 loss 3.242936\n",
            "batch 204 loss 3.832681\n",
            "batch 205 loss 2.877590\n",
            "batch 206 loss 4.137334\n",
            "batch 207 loss 2.080905\n",
            "batch 208 loss 3.795606\n",
            "batch 209 loss 3.171241\n",
            "batch 210 loss 5.744811\n",
            "batch 211 loss 3.852144\n",
            "batch 212 loss 2.085002\n",
            "batch 213 loss 4.984753\n",
            "batch 214 loss 3.578606\n",
            "batch 215 loss 4.733568\n",
            "batch 216 loss 4.628067\n",
            "batch 217 loss 2.278791\n",
            "batch 218 loss 3.021700\n",
            "batch 219 loss 2.599312\n",
            "batch 220 loss 4.918488\n",
            "batch 221 loss 3.651772\n",
            "batch 222 loss 2.485474\n",
            "batch 223 loss 3.327863\n",
            "batch 224 loss 2.892843\n",
            "batch 225 loss 3.604537\n",
            "batch 226 loss 3.973962\n",
            "batch 227 loss 6.002287\n",
            "batch 228 loss 3.042848\n",
            "batch 229 loss 3.555740\n",
            "batch 230 loss 3.951045\n",
            "batch 231 loss 2.469828\n",
            "batch 232 loss 3.304199\n",
            "batch 233 loss 3.451325\n",
            "batch 234 loss 2.702007\n",
            "batch 235 loss 2.392735\n",
            "batch 236 loss 2.283113\n",
            "batch 237 loss 3.716080\n",
            "batch 238 loss 2.527022\n",
            "batch 239 loss 3.652627\n",
            "batch 240 loss 4.014296\n",
            "batch 241 loss 2.001513\n",
            "batch 242 loss 4.788781\n",
            "batch 243 loss 3.240813\n",
            "batch 244 loss 3.304156\n",
            "batch 245 loss 3.879071\n",
            "batch 246 loss 3.473440\n",
            "batch 247 loss 2.881602\n",
            "batch 248 loss 2.504489\n",
            "batch 249 loss 3.371033\n",
            "batch 250 loss 2.844882\n",
            "batch 251 loss 3.906304\n",
            "batch 252 loss 4.743495\n",
            "batch 253 loss 3.937335\n",
            "batch 254 loss 1.839374\n",
            "batch 255 loss 5.238280\n",
            "batch 256 loss 3.126611\n",
            "batch 257 loss 3.024158\n",
            "batch 258 loss 4.527911\n",
            "batch 259 loss 6.681231\n",
            "batch 260 loss 3.836245\n",
            "batch 261 loss 2.837597\n",
            "batch 262 loss 3.322641\n",
            "batch 263 loss 3.630744\n",
            "batch 264 loss 4.175050\n",
            "batch 265 loss 3.512565\n",
            "batch 266 loss 2.208104\n",
            "batch 267 loss 2.545751\n",
            "batch 268 loss 2.563986\n",
            "batch 269 loss 4.577213\n",
            "batch 270 loss 3.195117\n",
            "batch 271 loss 3.510700\n",
            "batch 272 loss 2.987616\n",
            "batch 273 loss 5.761747\n",
            "batch 274 loss 2.791635\n",
            "batch 275 loss 3.386814\n",
            "batch 276 loss 2.255192\n",
            "batch 277 loss 2.745633\n",
            "batch 278 loss 2.707390\n",
            "batch 279 loss 3.102145\n",
            "batch 280 loss 2.395764\n",
            "batch 281 loss 2.448451\n",
            "batch 282 loss 2.937451\n",
            "batch 283 loss 3.381326\n",
            "batch 284 loss 2.781977\n",
            "batch 285 loss 3.729543\n",
            "batch 286 loss 2.128012\n",
            "batch 287 loss 6.984312\n",
            "batch 288 loss 5.354742\n",
            "batch 289 loss 5.288558\n",
            "batch 290 loss 3.434582\n",
            "batch 291 loss 3.504995\n",
            "batch 292 loss 4.066275\n",
            "batch 293 loss 3.412045\n",
            "batch 294 loss 2.268602\n",
            "batch 295 loss 3.564006\n",
            "batch 296 loss 2.444236\n",
            "batch 297 loss 2.313644\n",
            "batch 298 loss 3.479117\n",
            "batch 299 loss 2.354118\n",
            "batch 300 loss 2.446919\n",
            "batch 301 loss 2.415435\n",
            "batch 302 loss 3.055354\n",
            "batch 303 loss 3.875319\n",
            "batch 304 loss 2.133407\n",
            "batch 305 loss 2.845262\n",
            "batch 306 loss 3.800676\n",
            "batch 307 loss 3.265943\n",
            "batch 308 loss 2.915047\n",
            "batch 309 loss 3.229033\n",
            "batch 310 loss 4.062939\n",
            "batch 311 loss 2.080891\n",
            "batch 312 loss 3.521547\n",
            "batch 313 loss 3.070350\n",
            "batch 314 loss 2.025416\n",
            "batch 315 loss 3.497901\n",
            "batch 316 loss 2.930869\n",
            "batch 317 loss 2.297829\n",
            "batch 318 loss 1.865881\n",
            "batch 319 loss 3.330968\n",
            "batch 320 loss 3.471598\n",
            "batch 321 loss 4.873203\n",
            "batch 322 loss 3.410137\n",
            "batch 323 loss 1.828165\n",
            "batch 324 loss 2.282065\n",
            "batch 325 loss 3.202920\n",
            "batch 326 loss 4.030385\n",
            "batch 327 loss 2.941864\n",
            "batch 328 loss 2.632516\n",
            "batch 329 loss 1.765381\n",
            "batch 330 loss 3.490780\n",
            "batch 331 loss 3.499497\n",
            "batch 332 loss 3.090353\n",
            "batch 333 loss 3.671032\n",
            "batch 334 loss 4.956897\n",
            "batch 335 loss 3.074720\n",
            "batch 336 loss 1.732113\n",
            "batch 337 loss 3.109085\n",
            "batch 338 loss 2.963457\n",
            "batch 339 loss 4.305484\n",
            "batch 340 loss 3.421820\n",
            "batch 341 loss 2.507809\n",
            "batch 342 loss 1.804206\n",
            "batch 343 loss 4.160229\n",
            "batch 344 loss 1.755494\n",
            "batch 345 loss 3.408588\n",
            "batch 346 loss 2.923908\n",
            "batch 347 loss 1.704675\n",
            "batch 348 loss 2.912574\n",
            "batch 349 loss 1.647889\n",
            "batch 350 loss 1.625500\n",
            "batch 351 loss 3.350695\n",
            "batch 352 loss 3.310021\n",
            "batch 353 loss 2.846331\n",
            "batch 354 loss 3.343682\n",
            "batch 355 loss 5.194478\n",
            "batch 356 loss 2.850976\n",
            "batch 357 loss 1.424335\n",
            "batch 358 loss 3.189923\n",
            "batch 359 loss 3.099336\n",
            "batch 360 loss 3.650553\n",
            "batch 361 loss 1.355197\n",
            "batch 362 loss 2.713983\n",
            "batch 363 loss 2.630217\n",
            "batch 364 loss 2.825241\n",
            "batch 365 loss 2.936610\n",
            "batch 366 loss 3.690923\n",
            "batch 367 loss 3.182498\n",
            "batch 368 loss 2.984967\n",
            "batch 369 loss 3.642418\n",
            "batch 370 loss 2.600951\n",
            "batch 371 loss 2.515715\n",
            "batch 372 loss 1.503903\n",
            "batch 373 loss 3.924803\n",
            "batch 374 loss 3.466781\n",
            "batch 375 loss 2.495749\n",
            "batch 376 loss 8.050181\n",
            "batch 377 loss 2.783017\n",
            "batch 378 loss 3.357196\n",
            "batch 379 loss 3.087608\n",
            "batch 380 loss 4.853502\n",
            "batch 381 loss 3.491104\n",
            "batch 382 loss 2.335695\n",
            "batch 383 loss 1.847629\n",
            "batch 384 loss 1.856056\n",
            "batch 385 loss 3.249049\n",
            "batch 386 loss 2.707742\n",
            "batch 387 loss 3.958042\n",
            "batch 388 loss 1.789012\n",
            "batch 389 loss 1.769226\n",
            "batch 390 loss 3.924279\n",
            "batch 391 loss 2.910470\n",
            "batch 392 loss 5.134604\n",
            "batch 393 loss 1.771754\n",
            "batch 394 loss 3.157634\n",
            "batch 395 loss 1.631368\n",
            "batch 396 loss 2.304771\n",
            "batch 397 loss 3.536195\n",
            "batch 398 loss 2.630100\n",
            "batch 399 loss 5.078999\n",
            "batch 400 loss 4.601875\n",
            "batch 401 loss 4.895332\n",
            "batch 402 loss 1.606987\n",
            "batch 403 loss 2.288916\n",
            "batch 404 loss 3.374428\n",
            "batch 405 loss 3.563039\n",
            "batch 406 loss 1.515809\n",
            "batch 407 loss 3.183179\n",
            "batch 408 loss 2.236109\n",
            "batch 409 loss 3.302329\n",
            "batch 410 loss 3.156361\n",
            "batch 411 loss 2.204859\n",
            "batch 412 loss 4.391450\n",
            "batch 413 loss 3.175499\n",
            "batch 414 loss 4.278351\n",
            "batch 415 loss 3.072328\n",
            "batch 416 loss 3.453083\n",
            "batch 417 loss 1.664970\n",
            "batch 418 loss 3.137990\n",
            "batch 419 loss 4.657270\n",
            "batch 420 loss 2.142424\n",
            "batch 421 loss 3.336872\n",
            "batch 422 loss 2.747769\n",
            "batch 423 loss 3.605682\n",
            "batch 424 loss 3.426210\n",
            "batch 425 loss 1.743349\n",
            "batch 426 loss 1.705622\n",
            "batch 427 loss 3.756204\n",
            "batch 428 loss 1.711474\n",
            "batch 429 loss 4.132385\n",
            "batch 430 loss 3.064601\n",
            "batch 431 loss 2.634127\n",
            "batch 432 loss 2.649350\n",
            "batch 433 loss 1.700177\n",
            "batch 434 loss 1.651722\n",
            "batch 435 loss 2.142743\n",
            "batch 436 loss 3.184490\n",
            "batch 437 loss 4.517141\n",
            "batch 438 loss 3.210997\n",
            "batch 439 loss 2.161535\n",
            "batch 440 loss 3.120705\n",
            "batch 441 loss 4.790739\n",
            "batch 442 loss 3.104736\n",
            "batch 443 loss 1.478934\n",
            "batch 444 loss 2.934215\n",
            "batch 445 loss 2.893179\n",
            "batch 446 loss 3.383257\n",
            "batch 447 loss 1.529418\n",
            "batch 448 loss 3.272599\n",
            "batch 449 loss 2.464242\n",
            "batch 450 loss 2.957995\n",
            "batch 451 loss 3.156842\n",
            "batch 452 loss 1.518100\n",
            "batch 453 loss 2.212090\n",
            "batch 454 loss 4.216368\n",
            "batch 455 loss 3.856664\n",
            "batch 456 loss 4.110742\n",
            "batch 457 loss 3.296618\n",
            "batch 458 loss 3.059673\n",
            "batch 459 loss 3.785039\n",
            "batch 460 loss 2.968892\n",
            "batch 461 loss 3.529059\n",
            "batch 462 loss 3.628934\n",
            "batch 463 loss 3.538769\n",
            "batch 464 loss 1.664480\n",
            "batch 465 loss 2.592028\n",
            "batch 466 loss 2.478011\n",
            "batch 467 loss 2.563479\n",
            "batch 468 loss 2.293228\n",
            "batch 469 loss 3.107261\n",
            "batch 470 loss 1.677345\n",
            "batch 471 loss 5.602402\n",
            "batch 472 loss 2.777062\n",
            "batch 473 loss 2.415301\n",
            "batch 474 loss 3.355126\n",
            "batch 475 loss 2.665518\n",
            "batch 476 loss 2.387368\n",
            "batch 477 loss 2.742140\n",
            "batch 478 loss 5.228980\n",
            "batch 479 loss 3.892716\n",
            "batch 480 loss 3.198272\n",
            "batch 481 loss 1.969694\n",
            "batch 482 loss 3.366774\n",
            "batch 483 loss 4.644781\n",
            "batch 484 loss 1.916760\n",
            "batch 485 loss 3.176515\n",
            "batch 486 loss 2.265959\n",
            "batch 487 loss 2.516881\n",
            "batch 488 loss 3.108993\n",
            "batch 489 loss 4.453391\n",
            "batch 490 loss 1.942569\n",
            "batch 491 loss 1.802490\n",
            "batch 492 loss 2.284979\n",
            "batch 493 loss 2.617489\n",
            "batch 494 loss 3.919478\n",
            "batch 495 loss 3.066136\n",
            "batch 496 loss 2.712675\n",
            "batch 497 loss 1.833247\n",
            "batch 498 loss 3.799334\n",
            "batch 499 loss 2.664011\n",
            "batch 500 loss 4.065663\n",
            "batch 501 loss 2.569729\n",
            "batch 502 loss 4.129442\n",
            "batch 503 loss 2.921031\n",
            "batch 504 loss 2.546680\n",
            "batch 505 loss 3.407243\n",
            "batch 506 loss 2.586927\n",
            "batch 507 loss 1.821183\n",
            "batch 508 loss 5.226029\n",
            "batch 509 loss 3.492459\n",
            "batch 510 loss 4.638394\n",
            "batch 511 loss 2.918424\n",
            "batch 512 loss 3.300345\n",
            "batch 513 loss 2.403540\n",
            "batch 514 loss 2.358013\n",
            "batch 515 loss 2.402968\n",
            "batch 516 loss 1.979431\n",
            "batch 517 loss 2.482858\n",
            "batch 518 loss 4.381461\n",
            "batch 519 loss 3.211778\n",
            "batch 520 loss 3.227896\n",
            "batch 521 loss 2.907854\n",
            "batch 522 loss 2.850988\n",
            "batch 523 loss 2.920028\n",
            "batch 524 loss 3.266895\n",
            "batch 525 loss 2.773494\n",
            "batch 526 loss 3.735592\n",
            "batch 527 loss 3.208747\n",
            "batch 528 loss 3.641271\n",
            "batch 529 loss 2.658769\n",
            "batch 530 loss 2.037457\n",
            "batch 531 loss 2.911325\n",
            "batch 532 loss 3.389923\n",
            "batch 533 loss 2.611913\n",
            "batch 534 loss 2.373230\n",
            "batch 535 loss 4.279463\n",
            "batch 536 loss 4.881247\n",
            "batch 537 loss 2.592351\n",
            "batch 538 loss 2.355073\n",
            "batch 539 loss 2.593260\n",
            "batch 540 loss 2.600965\n",
            "batch 541 loss 4.579427\n",
            "batch 542 loss 2.876750\n",
            "batch 543 loss 3.858223\n",
            "batch 544 loss 2.566507\n",
            "batch 545 loss 2.544662\n",
            "batch 546 loss 4.149812\n",
            "batch 547 loss 2.549300\n",
            "batch 548 loss 3.951452\n",
            "batch 549 loss 2.659670\n",
            "batch 550 loss 4.419112\n",
            "batch 551 loss 2.526677\n",
            "batch 552 loss 2.516142\n",
            "batch 553 loss 2.570289\n",
            "batch 554 loss 2.360511\n",
            "batch 555 loss 3.925065\n",
            "batch 556 loss 2.429692\n",
            "batch 557 loss 2.438839\n",
            "batch 558 loss 2.662809\n",
            "batch 559 loss 2.497269\n",
            "batch 560 loss 2.339531\n",
            "batch 561 loss 2.348733\n",
            "batch 562 loss 4.835536\n",
            "batch 563 loss 2.304712\n",
            "batch 564 loss 3.503959\n",
            "batch 565 loss 2.134982\n",
            "batch 566 loss 2.101231\n",
            "batch 567 loss 2.558160\n",
            "batch 568 loss 2.513853\n",
            "batch 569 loss 2.530308\n",
            "batch 570 loss 3.693832\n",
            "batch 571 loss 1.921935\n",
            "batch 572 loss 3.697935\n",
            "batch 573 loss 2.382973\n",
            "batch 574 loss 2.483812\n",
            "batch 575 loss 2.684176\n",
            "batch 576 loss 1.887169\n",
            "batch 577 loss 3.390242\n",
            "batch 578 loss 2.639535\n",
            "batch 579 loss 2.366498\n",
            "batch 580 loss 1.797840\n",
            "batch 581 loss 2.157189\n",
            "batch 582 loss 5.158124\n",
            "batch 583 loss 2.492415\n",
            "batch 584 loss 2.575204\n",
            "batch 585 loss 2.243457\n",
            "batch 586 loss 2.152287\n",
            "batch 587 loss 2.368575\n",
            "batch 588 loss 1.618038\n",
            "batch 589 loss 3.674011\n",
            "batch 590 loss 2.221766\n",
            "batch 591 loss 2.487162\n",
            "batch 592 loss 2.417490\n",
            "batch 593 loss 2.433743\n",
            "batch 594 loss 2.314365\n",
            "batch 595 loss 2.393421\n",
            "batch 596 loss 3.528732\n",
            "batch 597 loss 4.385440\n",
            "batch 598 loss 2.238078\n",
            "batch 599 loss 2.183755\n",
            "batch 600 loss 3.745036\n",
            "batch 601 loss 3.710393\n",
            "batch 602 loss 5.523417\n",
            "batch 603 loss 4.238223\n",
            "batch 604 loss 3.347482\n",
            "batch 605 loss 4.403680\n",
            "batch 606 loss 2.408829\n",
            "batch 607 loss 3.204872\n",
            "batch 608 loss 4.110288\n",
            "batch 609 loss 2.086190\n",
            "batch 610 loss 4.240502\n",
            "batch 611 loss 4.837394\n",
            "batch 612 loss 3.874307\n",
            "batch 613 loss 2.115281\n",
            "batch 614 loss 3.826905\n",
            "batch 615 loss 3.619062\n",
            "batch 616 loss 3.421327\n",
            "batch 617 loss 5.413316\n",
            "batch 618 loss 2.445828\n",
            "batch 619 loss 2.403781\n",
            "batch 620 loss 4.315622\n",
            "batch 621 loss 2.202452\n",
            "batch 622 loss 2.506536\n",
            "batch 623 loss 2.509397\n",
            "batch 624 loss 2.280660\n",
            "batch 625 loss 2.456357\n",
            "batch 626 loss 4.287857\n",
            "batch 627 loss 3.633147\n",
            "batch 628 loss 3.603804\n",
            "batch 629 loss 2.179220\n",
            "batch 630 loss 3.336863\n",
            "batch 631 loss 2.319551\n",
            "batch 632 loss 2.269523\n",
            "batch 633 loss 3.445239\n",
            "batch 634 loss 7.868328\n",
            "batch 635 loss 4.103938\n",
            "batch 636 loss 4.078569\n",
            "batch 637 loss 2.213562\n",
            "batch 638 loss 2.381101\n",
            "batch 639 loss 2.905037\n",
            "batch 640 loss 3.061198\n",
            "batch 641 loss 3.700482\n",
            "batch 642 loss 4.049049\n",
            "batch 643 loss 3.767977\n",
            "batch 644 loss 3.724754\n",
            "batch 645 loss 2.519803\n",
            "batch 646 loss 3.232546\n",
            "batch 647 loss 2.315131\n",
            "batch 648 loss 3.590929\n",
            "batch 649 loss 4.309008\n",
            "batch 650 loss 3.331140\n",
            "batch 651 loss 4.226149\n",
            "batch 652 loss 2.887237\n",
            "batch 653 loss 3.093214\n",
            "batch 654 loss 3.871419\n",
            "batch 655 loss 3.174120\n",
            "batch 656 loss 2.511502\n",
            "batch 657 loss 2.493668\n",
            "batch 658 loss 3.158532\n",
            "batch 659 loss 3.006496\n",
            "batch 660 loss 3.118984\n",
            "batch 661 loss 3.264213\n",
            "batch 662 loss 3.231858\n",
            "batch 663 loss 2.797300\n",
            "batch 664 loss 3.793860\n",
            "batch 665 loss 2.555555\n",
            "batch 666 loss 2.767822\n",
            "batch 667 loss 2.582939\n",
            "batch 668 loss 2.900692\n",
            "batch 669 loss 2.495488\n",
            "batch 670 loss 2.465389\n",
            "batch 671 loss 2.623241\n",
            "batch 672 loss 2.882461\n",
            "batch 673 loss 3.275960\n",
            "batch 674 loss 2.563761\n",
            "batch 675 loss 3.205610\n",
            "batch 676 loss 4.215647\n",
            "batch 677 loss 3.111822\n",
            "batch 678 loss 2.454083\n",
            "batch 679 loss 3.387232\n",
            "batch 680 loss 2.425154\n",
            "batch 681 loss 4.057736\n",
            "batch 682 loss 4.207846\n",
            "batch 683 loss 3.671234\n",
            "batch 684 loss 7.858105\n",
            "batch 685 loss 2.794605\n",
            "batch 686 loss 2.794561\n",
            "batch 687 loss 2.217399\n",
            "batch 688 loss 3.331656\n",
            "batch 689 loss 3.093782\n",
            "batch 690 loss 3.608802\n",
            "batch 691 loss 2.521938\n",
            "batch 692 loss 2.664962\n",
            "batch 693 loss 3.868627\n",
            "batch 694 loss 2.730426\n",
            "batch 695 loss 2.617310\n",
            "batch 696 loss 4.069656\n",
            "batch 697 loss 2.666763\n",
            "batch 698 loss 3.147609\n",
            "batch 699 loss 2.531846\n",
            "batch 700 loss 7.676769\n",
            "batch 701 loss 2.630554\n",
            "batch 702 loss 2.801560\n",
            "batch 703 loss 3.066643\n",
            "batch 704 loss 2.686449\n",
            "batch 705 loss 2.560928\n",
            "batch 706 loss 3.912036\n",
            "batch 707 loss 4.923827\n",
            "batch 708 loss 4.857194\n",
            "batch 709 loss 4.740652\n",
            "batch 710 loss 2.303895\n",
            "batch 711 loss 2.658405\n",
            "batch 712 loss 3.144434\n",
            "batch 713 loss 2.770297\n",
            "batch 714 loss 2.786368\n",
            "batch 715 loss 2.725621\n",
            "batch 716 loss 4.401220\n",
            "batch 717 loss 2.625190\n",
            "batch 718 loss 3.782484\n",
            "batch 719 loss 2.440034\n",
            "batch 720 loss 3.383183\n",
            "batch 721 loss 2.725758\n",
            "batch 722 loss 2.781240\n",
            "batch 723 loss 3.052679\n",
            "batch 724 loss 4.475396\n",
            "batch 725 loss 2.778532\n",
            "batch 726 loss 3.264724\n",
            "batch 727 loss 2.620179\n",
            "batch 728 loss 3.889267\n",
            "batch 729 loss 3.797410\n",
            "batch 730 loss 2.942152\n",
            "batch 731 loss 2.350437\n",
            "batch 732 loss 2.654865\n",
            "batch 733 loss 2.578789\n",
            "batch 734 loss 2.602442\n",
            "batch 735 loss 3.182978\n",
            "batch 736 loss 3.124665\n",
            "batch 737 loss 2.569019\n",
            "batch 738 loss 3.160606\n",
            "batch 739 loss 3.799887\n",
            "batch 740 loss 2.529240\n",
            "batch 741 loss 2.979611\n",
            "batch 742 loss 3.214486\n",
            "batch 743 loss 3.251970\n",
            "batch 744 loss 2.509891\n",
            "batch 745 loss 2.550626\n",
            "batch 746 loss 2.356856\n",
            "batch 747 loss 2.849258\n",
            "batch 748 loss 2.514813\n",
            "batch 749 loss 2.230992\n",
            "batch 750 loss 3.004059\n",
            "batch 751 loss 3.272364\n",
            "batch 752 loss 2.822170\n",
            "batch 753 loss 2.354213\n",
            "batch 754 loss 2.166266\n",
            "batch 755 loss 2.448498\n",
            "batch 756 loss 4.019442\n",
            "batch 757 loss 2.775175\n",
            "batch 758 loss 4.016721\n",
            "batch 759 loss 2.907748\n",
            "batch 760 loss 3.969398\n",
            "batch 761 loss 4.546858\n",
            "batch 762 loss 3.103479\n",
            "batch 763 loss 3.861943\n",
            "batch 764 loss 4.355204\n",
            "batch 765 loss 2.582603\n",
            "batch 766 loss 2.749274\n",
            "batch 767 loss 2.164650\n",
            "batch 768 loss 3.272743\n",
            "batch 769 loss 2.602720\n",
            "batch 770 loss 2.626830\n",
            "batch 771 loss 2.210263\n",
            "batch 772 loss 2.633989\n",
            "batch 773 loss 2.413482\n",
            "batch 774 loss 2.766046\n",
            "batch 775 loss 3.146088\n",
            "batch 776 loss 2.849919\n",
            "batch 777 loss 2.150421\n",
            "batch 778 loss 2.710342\n",
            "batch 779 loss 4.593811\n",
            "batch 780 loss 3.220070\n",
            "batch 781 loss 7.488259\n",
            "batch 782 loss 2.060380\n",
            "batch 783 loss 3.850022\n",
            "batch 784 loss 2.818799\n",
            "batch 785 loss 2.176813\n",
            "batch 786 loss 2.787592\n",
            "batch 787 loss 2.679368\n",
            "batch 788 loss 3.143031\n",
            "batch 789 loss 4.362695\n",
            "batch 790 loss 2.743182\n",
            "batch 791 loss 2.029902\n",
            "batch 792 loss 2.801526\n",
            "batch 793 loss 2.434881\n",
            "batch 794 loss 3.117760\n",
            "batch 795 loss 2.947304\n",
            "batch 796 loss 2.612416\n",
            "batch 797 loss 2.667646\n",
            "batch 798 loss 3.045436\n",
            "batch 799 loss 1.999068\n",
            "batch 800 loss 1.992108\n",
            "batch 801 loss 1.941033\n",
            "batch 802 loss 5.154287\n",
            "batch 803 loss 3.180155\n",
            "batch 804 loss 1.808164\n",
            "batch 805 loss 1.899637\n",
            "batch 806 loss 7.311825\n",
            "batch 807 loss 2.711286\n",
            "batch 808 loss 2.882543\n",
            "batch 809 loss 2.654510\n",
            "batch 810 loss 2.581476\n",
            "batch 811 loss 2.618710\n",
            "batch 812 loss 2.905936\n",
            "batch 813 loss 4.163383\n",
            "batch 814 loss 2.538236\n",
            "batch 815 loss 6.805586\n",
            "batch 816 loss 2.964702\n",
            "batch 817 loss 2.735047\n",
            "batch 818 loss 2.757723\n",
            "batch 819 loss 2.425081\n",
            "batch 820 loss 2.682653\n",
            "batch 821 loss 2.640914\n",
            "batch 822 loss 2.912552\n",
            "batch 823 loss 2.361491\n",
            "batch 824 loss 2.445317\n",
            "batch 825 loss 2.611642\n",
            "batch 826 loss 2.244244\n",
            "batch 827 loss 1.981893\n",
            "batch 828 loss 4.498813\n",
            "batch 829 loss 2.355427\n",
            "batch 830 loss 2.365994\n",
            "batch 831 loss 2.766571\n",
            "batch 832 loss 2.765870\n",
            "batch 833 loss 3.351633\n",
            "batch 834 loss 4.129447\n",
            "batch 835 loss 2.100771\n",
            "batch 836 loss 1.924490\n",
            "batch 837 loss 2.764126\n",
            "batch 838 loss 2.054413\n",
            "batch 839 loss 3.241698\n",
            "batch 840 loss 2.039364\n",
            "batch 841 loss 2.769677\n",
            "batch 842 loss 6.931890\n",
            "batch 843 loss 4.237463\n",
            "batch 844 loss 3.322687\n",
            "batch 845 loss 2.243772\n",
            "batch 846 loss 1.903036\n",
            "batch 847 loss 2.693135\n",
            "batch 848 loss 1.864194\n",
            "batch 849 loss 3.242982\n",
            "batch 850 loss 2.940205\n",
            "batch 851 loss 3.208837\n",
            "batch 852 loss 1.910827\n",
            "batch 853 loss 1.805809\n",
            "batch 854 loss 2.613805\n",
            "batch 855 loss 1.743457\n",
            "batch 856 loss 3.870666\n",
            "batch 857 loss 1.963288\n",
            "batch 858 loss 2.942104\n",
            "batch 859 loss 1.664981\n",
            "batch 860 loss 2.935187\n",
            "batch 861 loss 2.930393\n",
            "batch 862 loss 1.710385\n",
            "batch 863 loss 2.567286\n",
            "batch 864 loss 4.281082\n",
            "batch 865 loss 1.643631\n",
            "batch 866 loss 4.470345\n",
            "batch 867 loss 3.089480\n",
            "batch 868 loss 3.056343\n",
            "batch 869 loss 1.394200\n",
            "batch 870 loss 1.920679\n",
            "batch 871 loss 1.941921\n",
            "batch 872 loss 1.481629\n",
            "batch 873 loss 1.472692\n",
            "batch 874 loss 3.228420\n",
            "batch 875 loss 2.716275\n",
            "batch 876 loss 4.433218\n",
            "batch 877 loss 3.048395\n",
            "batch 878 loss 3.151415\n",
            "batch 879 loss 3.480201\n",
            "batch 880 loss 1.454204\n",
            "batch 881 loss 1.449142\n",
            "batch 882 loss 3.178258\n",
            "batch 883 loss 2.926556\n",
            "batch 884 loss 3.347771\n",
            "batch 885 loss 2.919343\n",
            "batch 886 loss 4.701849\n",
            "batch 887 loss 2.961405\n",
            "batch 888 loss 3.589406\n",
            "batch 889 loss 2.863217\n",
            "batch 890 loss 3.060200\n",
            "batch 891 loss 4.217854\n",
            "batch 892 loss 2.718761\n",
            "batch 893 loss 1.391065\n",
            "batch 894 loss 2.675414\n",
            "batch 895 loss 1.514225\n",
            "batch 896 loss 1.576578\n",
            "batch 897 loss 2.732916\n",
            "batch 898 loss 1.385033\n",
            "batch 899 loss 2.626201\n",
            "batch 900 loss 2.520059\n",
            "batch 901 loss 2.598720\n",
            "batch 902 loss 1.443156\n",
            "batch 903 loss 2.390402\n",
            "batch 904 loss 3.455370\n",
            "batch 905 loss 1.373755\n",
            "batch 906 loss 3.043362\n",
            "batch 907 loss 1.477643\n",
            "batch 908 loss 2.304717\n",
            "batch 909 loss 1.314205\n",
            "batch 910 loss 4.273808\n",
            "batch 911 loss 3.183347\n",
            "batch 912 loss 1.253712\n",
            "batch 913 loss 2.256822\n",
            "batch 914 loss 2.356083\n",
            "batch 915 loss 1.283149\n",
            "batch 916 loss 1.166914\n",
            "batch 917 loss 4.333592\n",
            "batch 918 loss 5.809063\n",
            "batch 919 loss 3.286109\n",
            "batch 920 loss 9.363381\n",
            "batch 921 loss 2.766418\n",
            "batch 922 loss 2.258803\n",
            "batch 923 loss 4.726258\n",
            "batch 924 loss 3.108936\n",
            "batch 925 loss 4.072045\n",
            "batch 926 loss 2.168074\n",
            "batch 927 loss 3.297758\n",
            "batch 928 loss 1.339196\n",
            "batch 929 loss 2.922406\n",
            "batch 930 loss 2.100306\n",
            "batch 931 loss 3.162554\n",
            "batch 932 loss 3.312550\n",
            "batch 933 loss 2.784598\n",
            "batch 934 loss 1.936323\n",
            "batch 935 loss 4.590186\n",
            "batch 936 loss 1.528920\n",
            "batch 937 loss 3.137282\n",
            "batch 938 loss 3.978109\n",
            "batch 939 loss 3.979014\n",
            "batch 940 loss 2.647981\n",
            "batch 941 loss 1.854903\n",
            "batch 942 loss 2.888606\n",
            "batch 943 loss 3.190310\n",
            "batch 944 loss 2.785148\n",
            "batch 945 loss 3.159593\n",
            "batch 946 loss 3.782852\n",
            "batch 947 loss 3.183915\n",
            "batch 948 loss 1.797121\n",
            "batch 949 loss 3.271183\n",
            "batch 950 loss 3.004096\n",
            "batch 951 loss 3.068058\n",
            "batch 952 loss 2.552019\n",
            "batch 953 loss 2.652967\n",
            "batch 954 loss 3.035058\n",
            "batch 955 loss 1.956259\n",
            "batch 956 loss 8.648100\n",
            "batch 957 loss 2.501835\n",
            "batch 958 loss 2.891608\n",
            "batch 959 loss 3.972696\n",
            "batch 960 loss 3.184368\n",
            "batch 961 loss 4.154240\n",
            "batch 962 loss 1.897137\n",
            "batch 963 loss 3.671808\n",
            "batch 964 loss 3.644340\n",
            "batch 965 loss 2.438292\n",
            "batch 966 loss 2.072901\n",
            "batch 967 loss 3.148881\n",
            "batch 968 loss 2.011967\n",
            "batch 969 loss 4.377167\n",
            "batch 970 loss 2.987489\n",
            "batch 971 loss 2.170935\n",
            "batch 972 loss 1.940839\n",
            "batch 973 loss 2.119935\n",
            "batch 974 loss 2.357908\n",
            "batch 975 loss 2.022126\n",
            "batch 976 loss 3.714436\n",
            "batch 977 loss 3.065303\n",
            "batch 978 loss 1.941851\n",
            "batch 979 loss 2.816482\n",
            "batch 980 loss 2.321033\n",
            "batch 981 loss 6.668152\n",
            "batch 982 loss 2.281983\n",
            "batch 983 loss 5.457306\n",
            "batch 984 loss 1.847168\n",
            "batch 985 loss 4.395651\n",
            "batch 986 loss 2.952070\n",
            "batch 987 loss 2.933761\n",
            "batch 988 loss 3.549051\n",
            "batch 989 loss 1.841961\n",
            "batch 990 loss 1.992575\n",
            "batch 991 loss 2.234686\n",
            "batch 992 loss 3.055233\n",
            "batch 993 loss 3.223642\n",
            "batch 994 loss 1.810691\n",
            "batch 995 loss 3.041442\n",
            "batch 996 loss 2.157670\n",
            "batch 997 loss 2.118582\n",
            "batch 998 loss 3.209262\n",
            "batch 999 loss 2.026800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**預測**"
      ],
      "metadata": {
        "id": "QZvH6P13Hd4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self, inputts,temperature=1.):\n",
        "  batch_size, _ = tf.shape(inputs)\n",
        "  logits = self(inputs, from_logits=True)\n",
        "  prob = tf.nn.softmax(logits / temperature).numpy()\n",
        "\n",
        "  return np.array([np.random.choice(self.num_chars,p=prob[i,:]) for i in range(batch_size.numpy())])\n",
        "\n"
      ],
      "metadata": {
        "id": "nCU1ZIpNpygt"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_, _ = data_loader.get_batch(seq_length,1)\n",
        "\n",
        "for diversity in [2,5,10,12]:\n",
        "  X = X_\n",
        "  print(\"diversity %f:\" % diversity)\n",
        "  data_loader.indices_char = {}\n",
        "  for t in range(400):\n",
        "    y_pred = model.predict(X,diversity)\n",
        "    y_pred_tuple = tuple(y_pred[0])\n",
        "    print(data_loader.indices_char[y_pred_tuple],end='',flush=True)\n",
        "    X =np.concatenate([[X[:,1:]],np.expand_dims(y_pred,axis=1)],axis=-1)\n",
        "\n",
        "  print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "X9OIki4FKIJj",
        "outputId": "cf55e606-3a81-4043-ca33-c1ada731f7c1"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diversity 2.000000:\n",
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-12ad3992129e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_pred_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred_tuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (0.00639752, 0.13063784, 0.006484941, 0.0063026473, 0.00017869977, 0.00017223791, 0.00021469717, 0.009279634, 0.005307309, 0.002303239, 0.00024387865, 0.0017270035, 0.00027172855, 0.00016810776, 0.00017899374, 0.00019029414, 0.00021610956, 0.00016388719, 0.00019852919, 0.000194413, 0.0010260284, 0.0012742308, 0.0004602888, 0.0009888425, 0.00017144614, 0.00048149517, 0.00018964647, 0.05071715, 0.0028155586, 0.015223122, 0.03201601, 0.15430212, 0.009561348, 0.032315236, 0.046573434, 0.044060707, 0.0018502431, 0.0064054294, 0.04215372, 0.015303586, 0.045666844, 0.041934043, 0.009951488, 0.00047226594, 0.036738575, 0.051842958, 0.12700272, 0.018047402, 0.0062095486, 0.019214367, 0.0012004928, 0.011950358, 0.00019211348, 0.00023966633, 0.00018863779, 0.0002149962, 0.00021221246)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_loader.chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o3fJ8JATwC1",
        "outputId": "7c5218a4-1321-413c-acf8-68c2102ca722"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ä', 'æ', 'é', 'ë']\n"
          ]
        }
      ]
    }
  ]
}