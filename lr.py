# -*- coding: utf-8 -*-
"""LR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tZ7FvxryC2Tsvh_gFC94TLPkw2jiAg3s

**資料**
"""

import numpy as np

X_raw = np.array([2013,2014,2015,2016,2017],dtype=np.float32)
Y_raw = np.array([12000,14000,15000,16500,17500],dtype=np.float32)

X = (X_raw - X_raw.min())/(X_raw.max() - X_raw.min())
Y = (Y_raw - Y_raw.min())/(Y_raw.max() - Y_raw.min())

print(X,Y)

"""**tensorflow下的線性回歸**"""

import tensorflow as tf

X = tf.constant(X)
Y = tf.constant(Y)

a = tf.Variable(initial_value = 0.)
b = tf.Variable(initial_value = 0.)

Variable = [a,b]

num_epoch = 10000
optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)

for e in range(num_epoch):
  with tf.GradientTape() as tape:
    y_pred = a * X + b
    loss = tf.reduce_sum(tf.square(y_pred - Y))
  grads = tape.gradient(loss,Variable)
  optimizer.apply_gradients(grads_and_vars=zip(grads,Variable))
  print(a,b)

"""**Numpy下的線性回輝**"""

a,b =  0,0

num_eepoch = 10000
learning_rate = 1e-3

for e in range(num_eepoch):
  y_pred = a * X + b
  grad_a,grad_b = 2*(y_pred - Y).dot(X),2*(y_pred - Y).sum()

  a,b = a  - learning_rate*grad_a,b - learning_rate*grad_b
  print(a,b)